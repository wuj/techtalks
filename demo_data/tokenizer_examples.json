{"metadata":{"primaryTokenizer":{"label":"GPT-4","encoding":"cl100k_base","vocabSize":100277,"type":"BPE (byte-level)","jsLibrary":"js-tiktoken or gpt-tokenizer (cl100k_base)","note":"The JS demo should use the same encoding for live tokenization. Curated examples below were generated with this encoding and should produce identical results in the browser."},"comparisonTokenizers":{"gpt2":{"label":"GPT-2","vocabSize":50257,"type":"BPE (byte-level)"},"bert":{"label":"BERT (WordPiece)","vocabSize":30522,"type":"WordPiece"}}},"categories":{"subword_splitting":{"label":"Subword Splitting","description":"How words break into pieces","icon":"‚úÇÔ∏è","order":0},"names":{"label":"Names","description":"Personal and proper names","icon":"üë§","order":1},"code":{"label":"Code Snippets","description":"Programming languages and markup","icon":"üíª","order":2},"slang":{"label":"Slang & Internet","description":"Informal language and internet culture","icon":"üó£Ô∏è","order":3},"emoji_unicode":{"label":"Emoji & Unicode","description":"Emoji, special characters, multi-byte sequences","icon":"üé®","order":4},"multilingual":{"label":"Multilingual","description":"Non-English scripts and mixed languages","icon":"üåç","order":5},"numbers":{"label":"Numbers & Math","description":"Digits, arithmetic, numerical patterns","icon":"üî¢","order":6},"whitespace":{"label":"Whitespace & Formatting","description":"Spaces, newlines, indentation","icon":"‚¨ú","order":7},"edge_cases":{"label":"Edge Cases & Fun","description":"Unusual inputs and linguistic puzzles","icon":"üß©","order":8}},"examples":[{"text":"unbelievably","category":"subword_splitting","annotation":"A single English word splits into 3 tokens: 'un' + 'believ' + 'ably'. The tokenizer learned common prefixes and suffixes as separate pieces. This is the core surprise: transformers don't see 'words.'","isOpener":true,"primary":{"tokens":[{"id":359,"text":"un","displayText":"un","byteLen":2},{"id":32898,"text":"belie","displayText":"belie","byteLen":5},{"id":89234,"text":"vably","displayText":"vably","byteLen":5}],"tokenCount":3},"comparison":{"gpt2":{"tokens":[{"id":403,"text":"un"},{"id":6667,"text":"bel"},{"id":11203,"text":"iev"},{"id":1346,"text":"ably"}],"tokenCount":4},"bert":{"tokens":[{"id":4895,"text":"un"},{"id":8671,"text":"##bel"},{"id":2666,"text":"##ie"},{"id":3567,"text":"##va"},{"id":6321,"text":"##bly"}],"tokenCount":5}},"charCount":12,"compressionRatio":4.0},{"text":"internationalization","category":"subword_splitting","annotation":"A long word that decomposes into meaningful morphemes. Notice how the splits roughly correspond to linguistic structure: inter + national + ization.","isOpener":false,"primary":{"tokens":[{"id":98697,"text":"international","displayText":"international","byteLen":13},{"id":2065,"text":"ization","displayText":"ization","byteLen":7}],"tokenCount":2},"comparison":{"gpt2":{"tokens":[{"id":45609,"text":"international"},{"id":1634,"text":"ization"}],"tokenCount":2},"bert":{"tokens":[{"id":2248,"text":"international"},{"id":3989,"text":"##ization"}],"tokenCount":2}},"charCount":20,"compressionRatio":10.0},{"text":"Transformers are unbelievably powerful","category":"subword_splitting","annotation":"'Transformers' may stay as one token (it's common in training data), while 'unbelievably' splits just like before. Context doesn't change tokenization ‚Äî it's purely a pattern-matching step.","isOpener":false,"primary":{"tokens":[{"id":9140,"text":"Transform","displayText":"Transform","byteLen":9},{"id":388,"text":"ers","displayText":"ers","byteLen":3},{"id":527,"text":" are","displayText":" are","byteLen":4},{"id":40037,"text":" unbelie","displayText":" unbelie","byteLen":8},{"id":89234,"text":"vably","displayText":"vably","byteLen":5},{"id":8147,"text":" powerful","displayText":" powerful","byteLen":9}],"tokenCount":6},"comparison":{"gpt2":{"tokens":[{"id":41762,"text":"Transform"},{"id":364,"text":"ers"},{"id":389,"text":" are"},{"id":48943,"text":" unbelievably"},{"id":3665,"text":" powerful"}],"tokenCount":5},"bert":{"tokens":[{"id":19081,"text":"transformers"},{"id":2024,"text":"are"},{"id":4895,"text":"un"},{"id":8671,"text":"##bel"},{"id":2666,"text":"##ie"},{"id":3567,"text":"##va"},{"id":6321,"text":"##bly"},{"id":3928,"text":"powerful"}],"tokenCount":8}},"charCount":38,"compressionRatio":6.33},{"text":"antidisestablishmentarianism","category":"subword_splitting","annotation":"One of the longest English words. See how many subword pieces the tokenizer needs. Compare the token count to a simple word like 'cat.'","isOpener":false,"primary":{"tokens":[{"id":519,"text":"ant","displayText":"ant","byteLen":3},{"id":85342,"text":"idis","displayText":"idis","byteLen":4},{"id":34500,"text":"establish","displayText":"establish","byteLen":9},{"id":479,"text":"ment","displayText":"ment","byteLen":4},{"id":8997,"text":"arian","displayText":"arian","byteLen":5},{"id":2191,"text":"ism","displayText":"ism","byteLen":3}],"tokenCount":6},"comparison":{"gpt2":{"tokens":[{"id":415,"text":"ant"},{"id":29207,"text":"idis"},{"id":44390,"text":"establishment"},{"id":3699,"text":"arian"},{"id":1042,"text":"ism"}],"tokenCount":5},"bert":{"tokens":[{"id":3424,"text":"anti"},{"id":10521,"text":"##dis"},{"id":4355,"text":"##est"},{"id":7875,"text":"##ab"},{"id":13602,"text":"##lish"},{"id":3672,"text":"##ment"},{"id":12199,"text":"##arian"},{"id":2964,"text":"##ism"}],"tokenCount":8}},"charCount":28,"compressionRatio":4.67},{"text":"supercalifragilisticexpialidocious","category":"subword_splitting","annotation":"A fun nonsense word. The tokenizer has never seen this as a unit, so it falls back to smaller learned fragments. Pure vocabulary words get one token; rare words get shredded.","isOpener":false,"primary":{"tokens":[{"id":13066,"text":"sup","displayText":"sup","byteLen":3},{"id":3035,"text":"erc","displayText":"erc","byteLen":3},{"id":278,"text":"al","displayText":"al","byteLen":2},{"id":333,"text":"if","displayText":"if","byteLen":2},{"id":4193,"text":"rag","displayText":"rag","byteLen":3},{"id":321,"text":"il","displayText":"il","byteLen":2},{"id":4633,"text":"istic","displayText":"istic","byteLen":5},{"id":4683,"text":"exp","displayText":"exp","byteLen":3},{"id":532,"text":"ial","displayText":"ial","byteLen":3},{"id":307,"text":"id","displayText":"id","byteLen":2},{"id":78287,"text":"ocious","displayText":"ocious","byteLen":6}],"tokenCount":11},"comparison":{"gpt2":{"tokens":[{"id":16668,"text":"super"},{"id":9948,"text":"cal"},{"id":361,"text":"if"},{"id":22562,"text":"rag"},{"id":346,"text":"il"},{"id":396,"text":"ist"},{"id":501,"text":"ice"},{"id":42372,"text":"xp"},{"id":498,"text":"ial"},{"id":312,"text":"id"},{"id":32346,"text":"ocious"}],"tokenCount":11},"bert":{"tokens":[{"id":3565,"text":"super"},{"id":9289,"text":"##cal"},{"id":10128,"text":"##if"},{"id":29181,"text":"##rag"},{"id":24411,"text":"##ilis"},{"id":4588,"text":"##tic"},{"id":10288,"text":"##ex"},{"id":19312,"text":"##pia"},{"id":21273,"text":"##lid"},{"id":10085,"text":"##oc"},{"id":6313,"text":"##ious"}],"tokenCount":11}},"charCount":34,"compressionRatio":3.09},{"text":"The bank by the river was steep, but the bank approved her loan quickly.","category":"subword_splitting","annotation":"The running example for the entire series. Both instances of 'bank' get the SAME token ID ‚Äî the tokenizer has no way to distinguish meanings. That's the problem we solve in Session 2.","isOpener":false,"primary":{"tokens":[{"id":791,"text":"The","displayText":"The","byteLen":3},{"id":6201,"text":" bank","displayText":" bank","byteLen":5},{"id":555,"text":" by","displayText":" by","byteLen":3},{"id":279,"text":" the","displayText":" the","byteLen":4},{"id":15140,"text":" river","displayText":" river","byteLen":6},{"id":574,"text":" was","displayText":" was","byteLen":4},{"id":32366,"text":" steep","displayText":" steep","byteLen":6},{"id":11,"text":",","displayText":",","byteLen":1},{"id":719,"text":" but","displayText":" but","byteLen":4},{"id":279,"text":" the","displayText":" the","byteLen":4},{"id":6201,"text":" bank","displayText":" bank","byteLen":5},{"id":12054,"text":" approved","displayText":" approved","byteLen":9},{"id":1077,"text":" her","displayText":" her","byteLen":4},{"id":11941,"text":" loan","displayText":" loan","byteLen":5},{"id":6288,"text":" quickly","displayText":" quickly","byteLen":8},{"id":13,"text":".","displayText":".","byteLen":1}],"tokenCount":16},"comparison":{"gpt2":{"tokens":[{"id":464,"text":"The"},{"id":3331,"text":" bank"},{"id":416,"text":" by"},{"id":262,"text":" the"},{"id":7850,"text":" river"},{"id":373,"text":" was"},{"id":14559,"text":" steep"},{"id":11,"text":","},{"id":475,"text":" but"},{"id":262,"text":" the"},{"id":3331,"text":" bank"},{"id":6325,"text":" approved"},{"id":607,"text":" her"},{"id":8063,"text":" loan"},{"id":2952,"text":" quickly"},{"id":13,"text":"."}],"tokenCount":16},"bert":{"tokens":[{"id":1996,"text":"the"},{"id":2924,"text":"bank"},{"id":2011,"text":"by"},{"id":1996,"text":"the"},{"id":2314,"text":"river"},{"id":2001,"text":"was"},{"id":9561,"text":"steep"},{"id":1010,"text":","},{"id":2021,"text":"but"},{"id":1996,"text":"the"},{"id":2924,"text":"bank"},{"id":4844,"text":"approved"},{"id":2014,"text":"her"},{"id":5414,"text":"loan"},{"id":2855,"text":"quickly"},{"id":1012,"text":"."}],"tokenCount":16}},"charCount":72,"compressionRatio":4.5},{"text":"Elon Musk announced a new product","category":"names","annotation":"Famous names are common in training data and often tokenize cleanly. 'Elon' and 'Musk' are likely single tokens.","isOpener":false,"primary":{"tokens":[{"id":6719,"text":"El","displayText":"El","byteLen":2},{"id":263,"text":"on","displayText":"on","byteLen":2},{"id":40638,"text":" Musk","displayText":" Musk","byteLen":5},{"id":7376,"text":" announced","displayText":" announced","byteLen":10},{"id":264,"text":" a","displayText":" a","byteLen":2},{"id":502,"text":" new","displayText":" new","byteLen":4},{"id":2027,"text":" product","displayText":" product","byteLen":8}],"tokenCount":7},"comparison":{"gpt2":{"tokens":[{"id":9527,"text":"El"},{"id":261,"text":"on"},{"id":20119,"text":" Musk"},{"id":3414,"text":" announced"},{"id":257,"text":" a"},{"id":649,"text":" new"},{"id":1720,"text":" product"}],"tokenCount":7},"bert":{"tokens":[{"id":3449,"text":"el"},{"id":2239,"text":"##on"},{"id":14163,"text":"mu"},{"id":6711,"text":"##sk"},{"id":2623,"text":"announced"},{"id":1037,"text":"a"},{"id":2047,"text":"new"},{"id":4031,"text":"product"}],"tokenCount":8}},"charCount":33,"compressionRatio":4.71},{"text":"Schwarzenegger is a famous actor","category":"names","annotation":"Long or uncommon surnames split into subword pieces. The model doesn't 'know' this is a name ‚Äî it's just bytes.","isOpener":false,"primary":{"tokens":[{"id":31224,"text":"Sch","displayText":"Sch","byteLen":3},{"id":11710,"text":"war","displayText":"war","byteLen":3},{"id":5797,"text":"zen","displayText":"zen","byteLen":3},{"id":797,"text":"eg","displayText":"eg","byteLen":2},{"id":1414,"text":"ger","displayText":"ger","byteLen":3},{"id":374,"text":" is","displayText":" is","byteLen":3},{"id":264,"text":" a","displayText":" a","byteLen":2},{"id":11495,"text":" famous","displayText":" famous","byteLen":7},{"id":12360,"text":" actor","displayText":" actor","byteLen":6}],"tokenCount":9},"comparison":{"gpt2":{"tokens":[{"id":14874,"text":"Sch"},{"id":5767,"text":"war"},{"id":89,"text":"z"},{"id":44028,"text":"enegger"},{"id":318,"text":" is"},{"id":257,"text":" a"},{"id":5863,"text":" famous"},{"id":8674,"text":" actor"}],"tokenCount":8},"bert":{"tokens":[{"id":29058,"text":"schwarz"},{"id":8625,"text":"##ene"},{"id":13327,"text":"##gger"},{"id":2003,"text":"is"},{"id":1037,"text":"a"},{"id":3297,"text":"famous"},{"id":3364,"text":"actor"}],"tokenCount":7}},"charCount":32,"compressionRatio":3.56},{"text":"My friend Xiaoqiang works at DeepMind","category":"names","annotation":"Non-English names often fragment heavily. The tokenizer's vocabulary is biased toward English text patterns. This is a real equity concern.","isOpener":false,"primary":{"tokens":[{"id":5159,"text":"My","displayText":"My","byteLen":2},{"id":4333,"text":" friend","displayText":" friend","byteLen":7},{"id":66690,"text":" Xiao","displayText":" Xiao","byteLen":5},{"id":80,"text":"q","displayText":"q","byteLen":1},{"id":28323,"text":"iang","displayText":"iang","byteLen":4},{"id":4375,"text":" works","displayText":" works","byteLen":6},{"id":520,"text":" at","displayText":" at","byteLen":3},{"id":18682,"text":" Deep","displayText":" Deep","byteLen":5},{"id":70738,"text":"Mind","displayText":"Mind","byteLen":4}],"tokenCount":9},"comparison":{"gpt2":{"tokens":[{"id":3666,"text":"My"},{"id":1545,"text":" friend"},{"id":28249,"text":" Xiao"},{"id":80,"text":"q"},{"id":15483,"text":"iang"},{"id":2499,"text":" works"},{"id":379,"text":" at"},{"id":10766,"text":" Deep"},{"id":28478,"text":"Mind"}],"tokenCount":9},"bert":{"tokens":[{"id":2026,"text":"my"},{"id":2767,"text":"friend"},{"id":19523,"text":"xiao"},{"id":14702,"text":"##qi"},{"id":5654,"text":"##ang"},{"id":2573,"text":"works"},{"id":2012,"text":"at"},{"id":2784,"text":"deep"},{"id":23356,"text":"##mind"}],"tokenCount":9}},"charCount":37,"compressionRatio":4.11},{"text":"Uvuvwevwevwe Onyetenyevwe Ugwemubwem Ossas","category":"names","annotation":"The famous meme name. Watch how each syllable group becomes its own token (or multiple tokens). Extreme case of training-data rarity.","isOpener":false,"primary":{"tokens":[{"id":52,"text":"U","displayText":"U","byteLen":1},{"id":85,"text":"v","displayText":"v","byteLen":1},{"id":12328,"text":"uv","displayText":"uv","byteLen":2},{"id":906,"text":"we","displayText":"we","byteLen":2},{"id":85,"text":"v","displayText":"v","byteLen":1},{"id":906,"text":"we","displayText":"we","byteLen":2},{"id":85,"text":"v","displayText":"v","byteLen":1},{"id":906,"text":"we","displayText":"we","byteLen":2},{"id":1952,"text":" On","displayText":" On","byteLen":3},{"id":47492,"text":"yet","displayText":"yet","byteLen":3},{"id":33495,"text":"eny","displayText":"eny","byteLen":3},{"id":5230,"text":"ev","displayText":"ev","byteLen":2},{"id":906,"text":"we","displayText":"we","byteLen":2},{"id":47430,"text":" Ug","displayText":" Ug","byteLen":3},{"id":86,"text":"w","displayText":"w","byteLen":1},{"id":336,"text":"em","displayText":"em","byteLen":2},{"id":392,"text":"ub","displayText":"ub","byteLen":2},{"id":86,"text":"w","displayText":"w","byteLen":1},{"id":336,"text":"em","displayText":"em","byteLen":2},{"id":507,"text":" O","displayText":" O","byteLen":2},{"id":784,"text":"ss","displayText":"ss","byteLen":2},{"id":300,"text":"as","displayText":"as","byteLen":2}],"tokenCount":22},"comparison":{"gpt2":{"tokens":[{"id":52,"text":"U"},{"id":85,"text":"v"},{"id":14795,"text":"uv"},{"id":732,"text":"we"},{"id":85,"text":"v"},{"id":732,"text":"we"},{"id":85,"text":"v"},{"id":732,"text":"we"},{"id":1550,"text":" On"},{"id":25907,"text":"yet"},{"id":28558,"text":"eny"},{"id":1990,"text":"ev"},{"id":732,"text":"we"},{"id":24384,"text":" Ug"},{"id":86,"text":"w"},{"id":368,"text":"em"},{"id":549,"text":"ub"},{"id":86,"text":"w"},{"id":368,"text":"em"},{"id":45397,"text":" Oss"},{"id":292,"text":"as"}],"tokenCount":21},"bert":{"tokens":[{"id":23068,"text":"uv"},{"id":2226,"text":"##u"},{"id":2615,"text":"##v"},{"id":8545,"text":"##we"},{"id":2615,"text":"##v"},{"id":8545,"text":"##we"},{"id":2615,"text":"##v"},{"id":8545,"text":"##we"},{"id":2006,"text":"on"},{"id":6672,"text":"##ye"},{"id":6528,"text":"##ten"},{"id":17240,"text":"##yev"},{"id":8545,"text":"##we"},{"id":1057,"text":"u"},{"id":2290,"text":"##g"},{"id":8545,"text":"##we"},{"id":12274,"text":"##mu"},{"id":2497,"text":"##b"},{"id":8545,"text":"##we"},{"id":2213,"text":"##m"},{"id":9808,"text":"os"},{"id":20939,"text":"##sas"}],"tokenCount":22}},"charCount":42,"compressionRatio":1.91},{"text":"def fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)","category":"code","annotation":"Code is tokenized too! Notice: whitespace/indentation, parentheses, colons, and operators each become tokens. 'fibonacci' may split into sub-pieces. The model sees structure through tokens, not syntax rules.","isOpener":false,"primary":{"tokens":[{"id":755,"text":"def","displayText":"def","byteLen":3},{"id":76798,"text":" fibonacci","displayText":" fibonacci","byteLen":10},{"id":1471,"text":"(n","displayText":"(n","byteLen":2},{"id":997,"text":"):\n","displayText":"'):\\n'","byteLen":3},{"id":262,"text":"   ","displayText":"'   '","byteLen":3},{"id":422,"text":" if","displayText":" if","byteLen":3},{"id":308,"text":" n","displayText":" n","byteLen":2},{"id":2717,"text":" <=","displayText":" <=","byteLen":3},{"id":220,"text":" ","displayText":" ","byteLen":1},{"id":16,"text":"1","displayText":"1","byteLen":1},{"id":512,"text":":\n","displayText":"':\\n'","byteLen":2},{"id":286,"text":"       ","displayText":"'       '","byteLen":7},{"id":471,"text":" return","displayText":" return","byteLen":7},{"id":308,"text":" n","displayText":" n","byteLen":2},{"id":198,"text":"\n","displayText":"'\\n'","byteLen":1},{"id":262,"text":"   ","displayText":"'   '","byteLen":3},{"id":471,"text":" return","displayText":" return","byteLen":7},{"id":76798,"text":" fibonacci","displayText":" fibonacci","byteLen":10},{"id":1471,"text":"(n","displayText":"(n","byteLen":2},{"id":12,"text":"-","displayText":"-","byteLen":1},{"id":16,"text":"1","displayText":"1","byteLen":1},{"id":8,"text":")","displayText":")","byteLen":1},{"id":489,"text":" +","displayText":" +","byteLen":2},{"id":76798,"text":" fibonacci","displayText":" fibonacci","byteLen":10},{"id":1471,"text":"(n","displayText":"(n","byteLen":2},{"id":12,"text":"-","displayText":"-","byteLen":1},{"id":17,"text":"2","displayText":"2","byteLen":1},{"id":8,"text":")","displayText":")","byteLen":1}],"tokenCount":28},"comparison":{"gpt2":{"tokens":[{"id":4299,"text":"def"},{"id":12900,"text":" fib"},{"id":261,"text":"on"},{"id":44456,"text":"acci"},{"id":7,"text":"("},{"id":77,"text":"n"},{"id":2599,"text":"):"},{"id":198,"text":"\n"},{"id":220,"text":" "},{"id":220,"text":" "},{"id":220,"text":" "},{"id":611,"text":" if"},{"id":299,"text":" n"},{"id":19841,"text":" <="},{"id":352,"text":" 1"},{"id":25,"text":":"},{"id":198,"text":"\n"},{"id":220,"text":" "},{"id":220,"text":" "},{"id":220,"text":" "},{"id":220,"text":" "},{"id":220,"text":" "},{"id":220,"text":" "},{"id":220,"text":" "},{"id":1441,"text":" return"},{"id":299,"text":" n"},{"id":198,"text":"\n"},{"id":220,"text":" "},{"id":220,"text":" "},{"id":220,"text":" "},{"id":1441,"text":" return"},{"id":12900,"text":" fib"},{"id":261,"text":"on"},{"id":44456,"text":"acci"},{"id":7,"text":"("},{"id":77,"text":"n"},{"id":12,"text":"-"},{"id":16,"text":"1"},{"id":8,"text":")"},{"id":1343,"text":" +"},{"id":12900,"text":" fib"},{"id":261,"text":"on"},{"id":44456,"text":"acci"},{"id":7,"text":"("},{"id":77,"text":"n"},{"id":12,"text":"-"},{"id":17,"text":"2"},{"id":8,"text":")"}],"tokenCount":48},"bert":{"tokens":[{"id":13366,"text":"def"},{"id":10882,"text":"fi"},{"id":11735,"text":"##bon"},{"id":6305,"text":"##ac"},{"id":6895,"text":"##ci"},{"id":1006,"text":"("},{"id":1050,"text":"n"},{"id":1007,"text":")"},{"id":1024,"text":":"},{"id":2065,"text":"if"},{"id":1050,"text":"n"},{"id":1026,"text":"<"},{"id":1027,"text":"="},{"id":1015,"text":"1"},{"id":1024,"text":":"},{"id":2709,"text":"return"},{"id":1050,"text":"n"},{"id":2709,"text":"return"},{"id":10882,"text":"fi"},{"id":11735,"text":"##bon"},{"id":6305,"text":"##ac"},{"id":6895,"text":"##ci"},{"id":1006,"text":"("},{"id":1050,"text":"n"},{"id":1011,"text":"-"},{"id":1015,"text":"1"},{"id":1007,"text":")"},{"id":1009,"text":"+"},{"id":10882,"text":"fi"},{"id":11735,"text":"##bon"},{"id":6305,"text":"##ac"},{"id":6895,"text":"##ci"},{"id":1006,"text":"("},{"id":1050,"text":"n"},{"id":1011,"text":"-"},{"id":1016,"text":"2"},{"id":1007,"text":")"}],"tokenCount":37}},"charCount":92,"compressionRatio":3.29},{"text":"console.log('Hello, World!');","category":"code","annotation":"JavaScript one-liner. 'console', '.', 'log' might be separate tokens. The dot is significant ‚Äî it's a token just like any word.","isOpener":false,"primary":{"tokens":[{"id":5467,"text":"console","displayText":"console","byteLen":7},{"id":1699,"text":".log","displayText":".log","byteLen":4},{"id":493,"text":"('","displayText":"('","byteLen":2},{"id":9906,"text":"Hello","displayText":"Hello","byteLen":5},{"id":11,"text":",","displayText":",","byteLen":1},{"id":4435,"text":" World","displayText":" World","byteLen":6},{"id":0,"text":"!","displayText":"!","byteLen":1},{"id":4772,"text":"');","displayText":"');","byteLen":3}],"tokenCount":8},"comparison":{"gpt2":{"tokens":[{"id":41947,"text":"console"},{"id":13,"text":"."},{"id":6404,"text":"log"},{"id":10786,"text":"('"},{"id":15496,"text":"Hello"},{"id":11,"text":","},{"id":2159,"text":" World"},{"id":13679,"text":"!'"},{"id":1776,"text":");"}],"tokenCount":9},"bert":{"tokens":[{"id":10122,"text":"console"},{"id":1012,"text":"."},{"id":8833,"text":"log"},{"id":1006,"text":"("},{"id":1005,"text":"'"},{"id":7592,"text":"hello"},{"id":1010,"text":","},{"id":2088,"text":"world"},{"id":999,"text":"!"},{"id":1005,"text":"'"},{"id":1007,"text":")"},{"id":1025,"text":";"}],"tokenCount":12}},"charCount":29,"compressionRatio":3.62},{"text":"SELECT * FROM users WHERE age > 21 ORDER BY name;","category":"code","annotation":"SQL query. Keywords like SELECT, FROM, WHERE are common in training data and likely single tokens. Compare token count to equivalent English.","isOpener":false,"primary":{"tokens":[{"id":4963,"text":"SELECT","displayText":"SELECT","byteLen":6},{"id":353,"text":" *","displayText":" *","byteLen":2},{"id":4393,"text":" FROM","displayText":" FROM","byteLen":5},{"id":3932,"text":" users","displayText":" users","byteLen":6},{"id":5401,"text":" WHERE","displayText":" WHERE","byteLen":6},{"id":4325,"text":" age","displayText":" age","byteLen":4},{"id":871,"text":" >","displayText":" >","byteLen":2},{"id":220,"text":" ","displayText":" ","byteLen":1},{"id":1691,"text":"21","displayText":"21","byteLen":2},{"id":15888,"text":" ORDER","displayText":" ORDER","byteLen":6},{"id":7866,"text":" BY","displayText":" BY","byteLen":3},{"id":836,"text":" name","displayText":" name","byteLen":5},{"id":26,"text":";","displayText":";","byteLen":1}],"tokenCount":13},"comparison":{"gpt2":{"tokens":[{"id":46506,"text":"SELECT"},{"id":1635,"text":" *"},{"id":16034,"text":" FROM"},{"id":2985,"text":" users"},{"id":33411,"text":" WHERE"},{"id":2479,"text":" age"},{"id":1875,"text":" >"},{"id":2310,"text":" 21"},{"id":38678,"text":" ORDER"},{"id":11050,"text":" BY"},{"id":1438,"text":" name"},{"id":26,"text":";"}],"tokenCount":12},"bert":{"tokens":[{"id":7276,"text":"select"},{"id":1008,"text":"*"},{"id":2013,"text":"from"},{"id":5198,"text":"users"},{"id":2073,"text":"where"},{"id":2287,"text":"age"},{"id":1028,"text":">"},{"id":2538,"text":"21"},{"id":2344,"text":"order"},{"id":2011,"text":"by"},{"id":2171,"text":"name"},{"id":1025,"text":";"}],"tokenCount":12}},"charCount":49,"compressionRatio":3.77},{"text":"<div class=\"container\"><p>Hello</p></div>","category":"code","annotation":"HTML markup. Angle brackets, attribute names, and tag names each become tokens. The model must learn that '<div>' is a unit from token patterns.","isOpener":false,"primary":{"tokens":[{"id":2691,"text":"<div","displayText":"<div","byteLen":4},{"id":538,"text":" class","displayText":" class","byteLen":6},{"id":429,"text":"=\"","displayText":"=\"","byteLen":2},{"id":3670,"text":"container","displayText":"container","byteLen":9},{"id":3164,"text":"\"><","displayText":"\"><","byteLen":3},{"id":79,"text":"p","displayText":"p","byteLen":1},{"id":80597,"text":">Hello","displayText":">Hello","byteLen":6},{"id":524,"text":"</","displayText":"</","byteLen":2},{"id":79,"text":"p","displayText":"p","byteLen":1},{"id":1500,"text":"></","displayText":"></","byteLen":3},{"id":614,"text":"div","displayText":"div","byteLen":3},{"id":29,"text":">","displayText":">","byteLen":1}],"tokenCount":12},"comparison":{"gpt2":{"tokens":[{"id":27,"text":"<"},{"id":7146,"text":"div"},{"id":1398,"text":" class"},{"id":2625,"text":"=\""},{"id":34924,"text":"container"},{"id":22039,"text":"\"><"},{"id":79,"text":"p"},{"id":29,"text":">"},{"id":15496,"text":"Hello"},{"id":3556,"text":"</"},{"id":79,"text":"p"},{"id":12240,"text":"></"},{"id":7146,"text":"div"},{"id":29,"text":">"}],"tokenCount":14},"bert":{"tokens":[{"id":1026,"text":"<"},{"id":4487,"text":"di"},{"id":2615,"text":"##v"},{"id":2465,"text":"class"},{"id":1027,"text":"="},{"id":1000,"text":"\""},{"id":11661,"text":"container"},{"id":1000,"text":"\""},{"id":1028,"text":">"},{"id":1026,"text":"<"},{"id":1052,"text":"p"},{"id":1028,"text":">"},{"id":7592,"text":"hello"},{"id":1026,"text":"<"},{"id":1013,"text":"/"},{"id":1052,"text":"p"},{"id":1028,"text":">"},{"id":1026,"text":"<"},{"id":1013,"text":"/"},{"id":4487,"text":"di"},{"id":2615,"text":"##v"},{"id":1028,"text":">"}],"tokenCount":22}},"charCount":41,"compressionRatio":3.42},{"text":"lmaooo bruh that's literally so sus no cap fr fr","category":"slang","annotation":"Internet slang. Some terms (like 'lol') are common enough to be single tokens. Others fragment. Extended letters ('lmaooo') split differently than their normal form.","isOpener":false,"primary":{"tokens":[{"id":75,"text":"l","displayText":"l","byteLen":1},{"id":1764,"text":"ma","displayText":"ma","byteLen":2},{"id":39721,"text":"ooo","displayText":"ooo","byteLen":3},{"id":1437,"text":" br","displayText":" br","byteLen":3},{"id":12825,"text":"uh","displayText":"uh","byteLen":2},{"id":430,"text":" that","displayText":" that","byteLen":5},{"id":596,"text":"'s","displayText":"'s","byteLen":2},{"id":16280,"text":" literally","displayText":" literally","byteLen":10},{"id":779,"text":" so","displayText":" so","byteLen":3},{"id":4582,"text":" sus","displayText":" sus","byteLen":4},{"id":912,"text":" no","displayText":" no","byteLen":3},{"id":2107,"text":" cap","displayText":" cap","byteLen":4},{"id":1448,"text":" fr","displayText":" fr","byteLen":3},{"id":1448,"text":" fr","displayText":" fr","byteLen":3}],"tokenCount":14},"comparison":{"gpt2":{"tokens":[{"id":75,"text":"l"},{"id":2611,"text":"ma"},{"id":34160,"text":"ooo"},{"id":18145,"text":" bru"},{"id":71,"text":"h"},{"id":326,"text":" that"},{"id":338,"text":"'s"},{"id":7360,"text":" literally"},{"id":523,"text":" so"},{"id":2341,"text":" sus"},{"id":645,"text":" no"},{"id":1451,"text":" cap"},{"id":1216,"text":" fr"},{"id":1216,"text":" fr"}],"tokenCount":14},"bert":{"tokens":[{"id":1048,"text":"l"},{"id":2863,"text":"##ma"},{"id":9541,"text":"##oo"},{"id":2080,"text":"##o"},{"id":7987,"text":"br"},{"id":27225,"text":"##uh"},{"id":2008,"text":"that"},{"id":1005,"text":"'"},{"id":1055,"text":"s"},{"id":6719,"text":"literally"},{"id":2061,"text":"so"},{"id":10514,"text":"su"},{"id":2015,"text":"##s"},{"id":2053,"text":"no"},{"id":6178,"text":"cap"},{"id":10424,"text":"fr"},{"id":10424,"text":"fr"}],"tokenCount":17}},"charCount":48,"compressionRatio":3.43},{"text":"gonna wanna shoulda coulda","category":"slang","annotation":"Informal contractions. These might be single tokens (very common in casual text) or split. Compare with their formal versions.","isOpener":false,"primary":{"tokens":[{"id":11932,"text":"gon","displayText":"gon","byteLen":3},{"id":3458,"text":"na","displayText":"na","byteLen":2},{"id":33833,"text":" wanna","displayText":" wanna","byteLen":6},{"id":1288,"text":" should","displayText":" should","byteLen":7},{"id":64,"text":"a","displayText":"a","byteLen":1},{"id":1436,"text":" could","displayText":" could","byteLen":6},{"id":64,"text":"a","displayText":"a","byteLen":1}],"tokenCount":7},"comparison":{"gpt2":{"tokens":[{"id":70,"text":"g"},{"id":6415,"text":"onna"},{"id":18869,"text":" wanna"},{"id":815,"text":" should"},{"id":64,"text":"a"},{"id":714,"text":" could"},{"id":64,"text":"a"}],"tokenCount":7},"bert":{"tokens":[{"id":6069,"text":"gonna"},{"id":10587,"text":"wanna"},{"id":2323,"text":"should"},{"id":2050,"text":"##a"},{"id":2071,"text":"could"},{"id":2050,"text":"##a"}],"tokenCount":6}},"charCount":26,"compressionRatio":3.71},{"text":"¬Ø\\_(„ÉÑ)_/¬Ø","category":"slang","annotation":"ASCII art / kaomoji. The katakana character „ÉÑ and the special characters each become their own tokens. Non-ASCII gets expensive.","isOpener":false,"primary":{"tokens":[{"id":35085,"text":"¬Ø","displayText":"¬Ø","byteLen":2},{"id":59,"text":"\\","displayText":"\\","byteLen":1},{"id":8526,"text":"_(","displayText":"_(","byteLen":2},{"id":2845,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":2},{"id":226,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":27020,"text":")_","displayText":")_","byteLen":2},{"id":14,"text":"/","displayText":"/","byteLen":1},{"id":35085,"text":"¬Ø","displayText":"¬Ø","byteLen":2}],"tokenCount":8},"comparison":{"gpt2":{"tokens":[{"id":5196,"text":"¬Ø"},{"id":59,"text":"\\"},{"id":41052,"text":"_("},{"id":41115,"text":"„ÉÑ"},{"id":8,"text":")"},{"id":62,"text":"_"},{"id":14,"text":"/"},{"id":5196,"text":"¬Ø"}],"tokenCount":8},"bert":{"tokens":[{"id":100,"text":"[UNK]"},{"id":1032,"text":"\\"},{"id":1035,"text":"_"},{"id":1006,"text":"("},{"id":1712,"text":"„ÉÑ"},{"id":1007,"text":")"},{"id":1035,"text":"_"},{"id":1013,"text":"/"},{"id":100,"text":"[UNK]"}],"tokenCount":9}},"charCount":9,"compressionRatio":1.12},{"text":"I love pizza üçï and coding üíª","category":"emoji_unicode","annotation":"Each emoji can take multiple tokens because they're multi-byte UTF-8. A single emoji might cost 2-3 tokens. The English words are cheap.","isOpener":false,"primary":{"tokens":[{"id":40,"text":"I","displayText":"I","byteLen":1},{"id":3021,"text":" love","displayText":" love","byteLen":5},{"id":23317,"text":" pizza","displayText":" pizza","byteLen":6},{"id":11410,"text":" ÔøΩ","displayText":" ÔøΩ","byteLen":3},{"id":235,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":243,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":323,"text":" and","displayText":" and","byteLen":4},{"id":11058,"text":" coding","displayText":" coding","byteLen":7},{"id":64139,"text":" ÔøΩ","displayText":" ÔøΩ","byteLen":4},{"id":119,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1}],"tokenCount":10},"comparison":{"gpt2":{"tokens":[{"id":40,"text":"I"},{"id":1842,"text":" love"},{"id":14256,"text":" pizza"},{"id":12520,"text":" ÔøΩ"},{"id":235,"text":"ÔøΩ"},{"id":243,"text":"ÔøΩ"},{"id":290,"text":" and"},{"id":19617,"text":" coding"},{"id":12520,"text":" ÔøΩ"},{"id":240,"text":"ÔøΩ"},{"id":119,"text":"ÔøΩ"}],"tokenCount":11},"bert":{"tokens":[{"id":1045,"text":"i"},{"id":2293,"text":"love"},{"id":10733,"text":"pizza"},{"id":100,"text":"[UNK]"},{"id":1998,"text":"and"},{"id":16861,"text":"coding"},{"id":100,"text":"[UNK]"}],"tokenCount":7}},"charCount":27,"compressionRatio":2.7},{"text":"üá∫üá∏ üáØüáµ üá´üá∑ üáßüá∑","category":"emoji_unicode","annotation":"Flag emoji are composed of two regional indicator symbols each. Four flags might take 10+ tokens. Tokenizers are byte-level, not glyph-level.","isOpener":false,"primary":{"tokens":[{"id":9468,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":2},{"id":229,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":118,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":9468,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":2},{"id":229,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":116,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":11410,"text":" ÔøΩ","displayText":" ÔøΩ","byteLen":3},{"id":229,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":107,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":9468,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":2},{"id":229,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":113,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":11410,"text":" ÔøΩ","displayText":" ÔøΩ","byteLen":3},{"id":229,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":104,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":9468,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":2},{"id":229,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":115,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":11410,"text":" ÔøΩ","displayText":" ÔøΩ","byteLen":3},{"id":229,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":100,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":9468,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":2},{"id":229,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":115,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1}],"tokenCount":24},"comparison":{"gpt2":{"tokens":[{"id":8582,"text":"ÔøΩ"},{"id":229,"text":"ÔøΩ"},{"id":118,"text":"ÔøΩ"},{"id":8582,"text":"ÔøΩ"},{"id":229,"text":"ÔøΩ"},{"id":116,"text":"ÔøΩ"},{"id":12520,"text":" ÔøΩ"},{"id":229,"text":"ÔøΩ"},{"id":107,"text":"ÔøΩ"},{"id":8582,"text":"ÔøΩ"},{"id":229,"text":"ÔøΩ"},{"id":113,"text":"ÔøΩ"},{"id":12520,"text":" ÔøΩ"},{"id":229,"text":"ÔøΩ"},{"id":104,"text":"ÔøΩ"},{"id":8582,"text":"ÔøΩ"},{"id":229,"text":"ÔøΩ"},{"id":115,"text":"ÔøΩ"},{"id":12520,"text":" ÔøΩ"},{"id":229,"text":"ÔøΩ"},{"id":100,"text":"ÔøΩ"},{"id":8582,"text":"ÔøΩ"},{"id":229,"text":"ÔøΩ"},{"id":115,"text":"ÔøΩ"}],"tokenCount":24},"bert":{"tokens":[{"id":100,"text":"[UNK]"},{"id":100,"text":"[UNK]"},{"id":100,"text":"[UNK]"},{"id":100,"text":"[UNK]"}],"tokenCount":4}},"charCount":11,"compressionRatio":0.46},{"text":"üë®‚Äçüë©‚Äçüëß‚Äçüë¶","category":"emoji_unicode","annotation":"Family emoji: a single visible glyph composed of multiple code points joined by zero-width joiners. This ONE emoji could be 5+ tokens.","isOpener":false,"primary":{"tokens":[{"id":9468,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":2},{"id":239,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":101,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":378,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":2},{"id":235,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":9468,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":2},{"id":239,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":102,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":378,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":2},{"id":235,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":9468,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":2},{"id":239,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":100,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":378,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":2},{"id":235,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":9468,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":2},{"id":239,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":99,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1}],"tokenCount":18},"comparison":{"gpt2":{"tokens":[{"id":41840,"text":"ÔøΩ"},{"id":101,"text":"ÔøΩ"},{"id":447,"text":"ÔøΩ"},{"id":235,"text":"ÔøΩ"},{"id":41840,"text":"ÔøΩ"},{"id":102,"text":"ÔøΩ"},{"id":447,"text":"ÔøΩ"},{"id":235,"text":"ÔøΩ"},{"id":41840,"text":"ÔøΩ"},{"id":100,"text":"ÔøΩ"},{"id":447,"text":"ÔøΩ"},{"id":235,"text":"ÔøΩ"},{"id":41840,"text":"ÔøΩ"},{"id":99,"text":"ÔøΩ"}],"tokenCount":14},"bert":{"tokens":[{"id":100,"text":"[UNK]"}],"tokenCount":1}},"charCount":7,"compressionRatio":0.39},{"text":"„Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå","category":"multilingual","annotation":"Japanese ('Hello World'). Each character may become its own token or even split into multiple tokens. English-dominant training data means non-Latin scripts are tokenized less efficiently.","isOpener":false,"primary":{"tokens":[{"id":90115,"text":"„Åì„Çì„Å´„Å°„ÅØ","displayText":"„Åì„Çì„Å´„Å°„ÅØ","byteLen":15},{"id":3574,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":2},{"id":244,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":98220,"text":"Áïå","displayText":"Áïå","byteLen":3}],"tokenCount":4},"comparison":{"gpt2":{"tokens":[{"id":46036,"text":"„Åì"},{"id":22174,"text":"„Çì"},{"id":28618,"text":"„Å´"},{"id":2515,"text":"ÔøΩ"},{"id":94,"text":"ÔøΩ"},{"id":31676,"text":"„ÅØ"},{"id":10310,"text":"ÔøΩ"},{"id":244,"text":"ÔøΩ"},{"id":45911,"text":"ÔøΩ"},{"id":234,"text":"ÔøΩ"}],"tokenCount":10},"bert":{"tokens":[{"id":1655,"text":"„Åì"},{"id":30217,"text":"##„Çì"},{"id":30194,"text":"##„Å´"},{"id":30188,"text":"##„Å°"},{"id":30198,"text":"##„ÅØ"},{"id":1745,"text":"‰∏ñ"},{"id":100,"text":"[UNK]"}],"tokenCount":7}},"charCount":7,"compressionRatio":1.75},{"text":"The word for cat in Spanish is gato and in Japanese is Áå´","category":"multilingual","annotation":"Mixed-script sentence. English flows cheaply, then the single kanji 'Áå´' costs disproportionately more tokens. Efficiency inequality across languages is a real issue.","isOpener":false,"primary":{"tokens":[{"id":791,"text":"The","displayText":"The","byteLen":3},{"id":3492,"text":" word","displayText":" word","byteLen":5},{"id":369,"text":" for","displayText":" for","byteLen":4},{"id":8415,"text":" cat","displayText":" cat","byteLen":4},{"id":304,"text":" in","displayText":" in","byteLen":3},{"id":15506,"text":" Spanish","displayText":" Spanish","byteLen":8},{"id":374,"text":" is","displayText":" is","byteLen":3},{"id":342,"text":" g","displayText":" g","byteLen":2},{"id":4428,"text":"ato","displayText":"ato","byteLen":3},{"id":323,"text":" and","displayText":" and","byteLen":4},{"id":304,"text":" in","displayText":" in","byteLen":3},{"id":11002,"text":" Japanese","displayText":" Japanese","byteLen":9},{"id":374,"text":" is","displayText":" is","byteLen":3},{"id":10447,"text":" ÔøΩ","displayText":" ÔøΩ","byteLen":2},{"id":234,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1},{"id":104,"text":"ÔøΩ","displayText":"ÔøΩ","byteLen":1}],"tokenCount":16},"comparison":{"gpt2":{"tokens":[{"id":464,"text":"The"},{"id":1573,"text":" word"},{"id":329,"text":" for"},{"id":3797,"text":" cat"},{"id":287,"text":" in"},{"id":7897,"text":" Spanish"},{"id":318,"text":" is"},{"id":308,"text":" g"},{"id":5549,"text":"ato"},{"id":290,"text":" and"},{"id":287,"text":" in"},{"id":4960,"text":" Japanese"},{"id":318,"text":" is"},{"id":13328,"text":" ÔøΩ"},{"id":234,"text":"ÔøΩ"},{"id":104,"text":"ÔøΩ"}],"tokenCount":16},"bert":{"tokens":[{"id":1996,"text":"the"},{"id":2773,"text":"word"},{"id":2005,"text":"for"},{"id":4937,"text":"cat"},{"id":1999,"text":"in"},{"id":3009,"text":"spanish"},{"id":2003,"text":"is"},{"id":11721,"text":"ga"},{"id":3406,"text":"##to"},{"id":1998,"text":"and"},{"id":1999,"text":"in"},{"id":2887,"text":"japanese"},{"id":2003,"text":"is"},{"id":100,"text":"[UNK]"}],"tokenCount":14}},"charCount":56,"compressionRatio":3.5},{"text":"–ü—Ä–∏–≤–µ—Ç –º–∏—Ä","category":"multilingual","annotation":"Russian ('Hello World'). Cyrillic characters are less frequent in training data than Latin, so they may fragment more.","isOpener":false,"primary":{"tokens":[{"id":54745,"text":"–ü—Ä","displayText":"–ü—Ä","byteLen":4},{"id":28089,"text":"–∏–≤","displayText":"–∏–≤","byteLen":4},{"id":8341,"text":"–µ—Ç","displayText":"–µ—Ç","byteLen":4},{"id":11562,"text":" –º","displayText":" –º","byteLen":3},{"id":78746,"text":"–∏—Ä","displayText":"–∏—Ä","byteLen":4}],"tokenCount":5},"comparison":{"gpt2":{"tokens":[{"id":140,"text":"ÔøΩ"},{"id":253,"text":"ÔøΩ"},{"id":21169,"text":"—Ä"},{"id":18849,"text":"–∏"},{"id":38857,"text":"–≤"},{"id":16843,"text":"–µ"},{"id":20375,"text":"—Ç"},{"id":12466,"text":" ÔøΩ"},{"id":120,"text":"ÔøΩ"},{"id":18849,"text":"–∏"},{"id":21169,"text":"—Ä"}],"tokenCount":11},"bert":{"tokens":[{"id":1194,"text":"–ø"},{"id":16856,"text":"##—Ä"},{"id":10325,"text":"##–∏"},{"id":25529,"text":"##–≤"},{"id":15290,"text":"##–µ"},{"id":22919,"text":"##—Ç"},{"id":1191,"text":"–º"},{"id":10325,"text":"##–∏"},{"id":16856,"text":"##—Ä"}],"tokenCount":9}},"charCount":10,"compressionRatio":2.0},{"text":"ŸÖÿ±ÿ≠ÿ®ÿß ÿ®ÿßŸÑÿπÿßŸÑŸÖ","category":"multilingual","annotation":"Arabic ('Hello World'). Right-to-left script with connecting letters. The tokenizer works on bytes, so directionality doesn't matter ‚Äî but vocabulary coverage does.","isOpener":false,"primary":{"tokens":[{"id":10386,"text":"ŸÖ","displayText":"ŸÖ","byteLen":2},{"id":11318,"text":"ÿ±","displayText":"ÿ±","byteLen":2},{"id":30925,"text":"ÿ≠","displayText":"ÿ≠","byteLen":2},{"id":22071,"text":"ÿ®","displayText":"ÿ®","byteLen":2},{"id":5821,"text":"ÿß","displayText":"ÿß","byteLen":2},{"id":28946,"text":" ÿ®","displayText":" ÿ®","byteLen":3},{"id":32482,"text":"ÿßŸÑ","displayText":"ÿßŸÑ","byteLen":4},{"id":24102,"text":"ÿπ","displayText":"ÿπ","byteLen":2},{"id":32482,"text":"ÿßŸÑ","displayText":"ÿßŸÑ","byteLen":4},{"id":10386,"text":"ŸÖ","displayText":"ŸÖ","byteLen":2}],"tokenCount":10},"comparison":{"gpt2":{"tokens":[{"id":25405,"text":"ŸÖ"},{"id":26897,"text":"ÿ±"},{"id":148,"text":"ÔøΩ"},{"id":255,"text":"ÔøΩ"},{"id":39848,"text":"ÿ®"},{"id":12919,"text":"ÿß"},{"id":17550,"text":" ÔøΩ"},{"id":101,"text":"ÔøΩ"},{"id":23525,"text":"ÿßŸÑ"},{"id":44690,"text":"ÿπ"},{"id":23525,"text":"ÿßŸÑ"},{"id":25405,"text":"ŸÖ"}],"tokenCount":12},"bert":{"tokens":[{"id":1295,"text":"ŸÖ"},{"id":17149,"text":"##ÿ±"},{"id":29820,"text":"##ÿ≠"},{"id":29816,"text":"##ÿ®"},{"id":25573,"text":"##ÿß"},{"id":1271,"text":"ÿ®"},{"id":25573,"text":"##ÿß"},{"id":23673,"text":"##ŸÑ"},{"id":29830,"text":"##ÿπ"},{"id":25573,"text":"##ÿß"},{"id":23673,"text":"##ŸÑ"},{"id":22192,"text":"##ŸÖ"}],"tokenCount":12}},"charCount":13,"compressionRatio":1.3},{"text":"The answer is 3.14159265358979","category":"numbers","annotation":"Numbers often split digit-by-digit or in small groups. '3.14' might be one token, but the remaining digits fragment. This is why LLMs struggle with precise arithmetic.","isOpener":false,"primary":{"tokens":[{"id":791,"text":"The","displayText":"The","byteLen":3},{"id":4320,"text":" answer","displayText":" answer","byteLen":7},{"id":374,"text":" is","displayText":" is","byteLen":3},{"id":220,"text":" ","displayText":" ","byteLen":1},{"id":18,"text":"3","displayText":"3","byteLen":1},{"id":13,"text":".","displayText":".","byteLen":1},{"id":9335,"text":"141","displayText":"141","byteLen":3},{"id":20128,"text":"592","displayText":"592","byteLen":3},{"id":21598,"text":"653","displayText":"653","byteLen":3},{"id":22905,"text":"589","displayText":"589","byteLen":3},{"id":4643,"text":"79","displayText":"79","byteLen":2}],"tokenCount":11},"comparison":{"gpt2":{"tokens":[{"id":464,"text":"The"},{"id":3280,"text":" answer"},{"id":318,"text":" is"},{"id":513,"text":" 3"},{"id":13,"text":"."},{"id":1415,"text":"14"},{"id":19707,"text":"159"},{"id":22980,"text":"265"},{"id":2327,"text":"35"},{"id":4531,"text":"89"},{"id":3720,"text":"79"}],"tokenCount":11},"bert":{"tokens":[{"id":1996,"text":"the"},{"id":3437,"text":"answer"},{"id":2003,"text":"is"},{"id":1017,"text":"3"},{"id":1012,"text":"."},{"id":15471,"text":"141"},{"id":28154,"text":"##59"},{"id":23833,"text":"##26"},{"id":22275,"text":"##53"},{"id":27814,"text":"##58"},{"id":2683,"text":"##9"},{"id":2581,"text":"##7"},{"id":2683,"text":"##9"}],"tokenCount":13}},"charCount":30,"compressionRatio":2.73},{"text":"My phone number is 555-867-5309","category":"numbers","annotation":"Phone number. Hyphens, digit groups ‚Äî each piece becomes a token. The model has no concept of 'phone number' at the token level.","isOpener":false,"primary":{"tokens":[{"id":5159,"text":"My","displayText":"My","byteLen":2},{"id":4641,"text":" phone","displayText":" phone","byteLen":6},{"id":1396,"text":" number","displayText":" number","byteLen":7},{"id":374,"text":" is","displayText":" is","byteLen":3},{"id":220,"text":" ","displayText":" ","byteLen":1},{"id":14148,"text":"555","displayText":"555","byteLen":3},{"id":12,"text":"-","displayText":"-","byteLen":1},{"id":26013,"text":"867","displayText":"867","byteLen":3},{"id":12,"text":"-","displayText":"-","byteLen":1},{"id":17252,"text":"530","displayText":"530","byteLen":3},{"id":24,"text":"9","displayText":"9","byteLen":1}],"tokenCount":11},"comparison":{"gpt2":{"tokens":[{"id":3666,"text":"My"},{"id":3072,"text":" phone"},{"id":1271,"text":" number"},{"id":318,"text":" is"},{"id":44717,"text":" 555"},{"id":12,"text":"-"},{"id":23,"text":"8"},{"id":3134,"text":"67"},{"id":12,"text":"-"},{"id":20,"text":"5"},{"id":26895,"text":"309"}],"tokenCount":11},"bert":{"tokens":[{"id":2026,"text":"my"},{"id":3042,"text":"phone"},{"id":2193,"text":"number"},{"id":2003,"text":"is"},{"id":29541,"text":"555"},{"id":1011,"text":"-"},{"id":6564,"text":"86"},{"id":2581,"text":"##7"},{"id":1011,"text":"-"},{"id":23523,"text":"530"},{"id":2683,"text":"##9"}],"tokenCount":11}},"charCount":31,"compressionRatio":2.82},{"text":"2 + 2 = 4 but 23847 * 9182 = ???","category":"numbers","annotation":"Simple arithmetic vs. complex. Single digits are single tokens, but multi-digit numbers split. Imagine doing multiplication when the digits of each number are scattered across different tokens.","isOpener":false,"primary":{"tokens":[{"id":17,"text":"2","displayText":"2","byteLen":1},{"id":489,"text":" +","displayText":" +","byteLen":2},{"id":220,"text":" ","displayText":" ","byteLen":1},{"id":17,"text":"2","displayText":"2","byteLen":1},{"id":284,"text":" =","displayText":" =","byteLen":2},{"id":220,"text":" ","displayText":" ","byteLen":1},{"id":19,"text":"4","displayText":"4","byteLen":1},{"id":719,"text":" but","displayText":" but","byteLen":4},{"id":220,"text":" ","displayText":" ","byteLen":1},{"id":13895,"text":"238","displayText":"238","byteLen":3},{"id":2618,"text":"47","displayText":"47","byteLen":2},{"id":353,"text":" *","displayText":" *","byteLen":2},{"id":220,"text":" ","displayText":" ","byteLen":1},{"id":25828,"text":"918","displayText":"918","byteLen":3},{"id":17,"text":"2","displayText":"2","byteLen":1},{"id":284,"text":" =","displayText":" =","byteLen":2},{"id":52417,"text":" ???","displayText":" ???","byteLen":4}],"tokenCount":17},"comparison":{"gpt2":{"tokens":[{"id":17,"text":"2"},{"id":1343,"text":" +"},{"id":362,"text":" 2"},{"id":796,"text":" ="},{"id":604,"text":" 4"},{"id":475,"text":" but"},{"id":32544,"text":" 238"},{"id":2857,"text":"47"},{"id":1635,"text":" *"},{"id":860,"text":" 9"},{"id":24294,"text":"182"},{"id":796,"text":" ="},{"id":34913,"text":" ???"}],"tokenCount":13},"bert":{"tokens":[{"id":1016,"text":"2"},{"id":1009,"text":"+"},{"id":1016,"text":"2"},{"id":1027,"text":"="},{"id":1018,"text":"4"},{"id":2021,"text":"but"},{"id":22030,"text":"238"},{"id":22610,"text":"##47"},{"id":1008,"text":"*"},{"id":6205,"text":"91"},{"id":2620,"text":"##8"},{"id":2475,"text":"##2"},{"id":1027,"text":"="},{"id":1029,"text":"?"},{"id":1029,"text":"?"},{"id":1029,"text":"?"}],"tokenCount":16}},"charCount":32,"compressionRatio":1.88},{"text":"word     word","category":"whitespace","annotation":"Multiple spaces. Some tokenizers encode runs of spaces as single tokens (efficient for code indentation). Others split them.","isOpener":false,"primary":{"tokens":[{"id":1178,"text":"word","displayText":"word","byteLen":4},{"id":257,"text":"    ","displayText":"'    '","byteLen":4},{"id":3492,"text":" word","displayText":" word","byteLen":5}],"tokenCount":3},"comparison":{"gpt2":{"tokens":[{"id":4775,"text":"word"},{"id":220,"text":" "},{"id":220,"text":" "},{"id":220,"text":" "},{"id":220,"text":" "},{"id":1573,"text":" word"}],"tokenCount":6},"bert":{"tokens":[{"id":2773,"text":"word"},{"id":2773,"text":"word"}],"tokenCount":2}},"charCount":13,"compressionRatio":4.33},{"text":"Line one.\nLine two.\n\nParagraph break.","category":"whitespace","annotation":"Newlines and paragraph breaks become tokens. The model 'sees' document structure through these whitespace tokens.","isOpener":false,"primary":{"tokens":[{"id":2519,"text":"Line","displayText":"Line","byteLen":4},{"id":832,"text":" one","displayText":" one","byteLen":4},{"id":627,"text":".\n","displayText":"'.\\n'","byteLen":2},{"id":2519,"text":"Line","displayText":"Line","byteLen":4},{"id":1403,"text":" two","displayText":" two","byteLen":4},{"id":382,"text":".\n\n","displayText":"'.\\n\\n'","byteLen":3},{"id":43265,"text":"Paragraph","displayText":"Paragraph","byteLen":9},{"id":1464,"text":" break","displayText":" break","byteLen":6},{"id":13,"text":".","displayText":".","byteLen":1}],"tokenCount":9},"comparison":{"gpt2":{"tokens":[{"id":13949,"text":"Line"},{"id":530,"text":" one"},{"id":13,"text":"."},{"id":198,"text":"\n"},{"id":13949,"text":"Line"},{"id":734,"text":" two"},{"id":13,"text":"."},{"id":198,"text":"\n"},{"id":198,"text":"\n"},{"id":10044,"text":"Par"},{"id":6111,"text":"agraph"},{"id":2270,"text":" break"},{"id":13,"text":"."}],"tokenCount":13},"bert":{"tokens":[{"id":2240,"text":"line"},{"id":2028,"text":"one"},{"id":1012,"text":"."},{"id":2240,"text":"line"},{"id":2048,"text":"two"},{"id":1012,"text":"."},{"id":20423,"text":"paragraph"},{"id":3338,"text":"break"},{"id":1012,"text":"."}],"tokenCount":9}},"charCount":37,"compressionRatio":4.11},{"text":"aaaaaaaaaaaaaaaa","category":"edge_cases","annotation":"Repeated character. Does the tokenizer compress this? Or does every character become its own token? BPE merges common pairs, so repeated letters often merge into chunks.","isOpener":false,"primary":{"tokens":[{"id":70540,"text":"aaaaaaaa","displayText":"aaaaaaaa","byteLen":8},{"id":70540,"text":"aaaaaaaa","displayText":"aaaaaaaa","byteLen":8}],"tokenCount":2},"comparison":{"gpt2":{"tokens":[{"id":24794,"text":"aaaa"},{"id":24794,"text":"aaaa"},{"id":24794,"text":"aaaa"},{"id":24794,"text":"aaaa"}],"tokenCount":4},"bert":{"tokens":[{"id":13360,"text":"aaa"},{"id":11057,"text":"##aa"},{"id":11057,"text":"##aa"},{"id":11057,"text":"##aa"},{"id":11057,"text":"##aa"},{"id":11057,"text":"##aa"},{"id":11057,"text":"##aa"},{"id":2050,"text":"##a"}],"tokenCount":8}},"charCount":16,"compressionRatio":8.0},{"text":"a","category":"edge_cases","annotation":"The minimal input: a single character. Still one token with a token ID. The model treats it the same as any other token.","isOpener":false,"primary":{"tokens":[{"id":64,"text":"a","displayText":"a","byteLen":1}],"tokenCount":1},"comparison":{"gpt2":{"tokens":[{"id":64,"text":"a"}],"tokenCount":1},"bert":{"tokens":[{"id":1037,"text":"a"}],"tokenCount":1}},"charCount":1,"compressionRatio":1.0},{"text":"","category":"edge_cases","annotation":"Empty string. Zero tokens. The tokenizer handles this gracefully.","isOpener":false,"primary":{"tokens":[],"tokenCount":0},"comparison":{"gpt2":{"tokens":[],"tokenCount":0},"bert":{"tokens":[],"tokenCount":0}},"charCount":0,"compressionRatio":0.0},{"text":"Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo","category":"edge_cases","annotation":"Grammatically correct English sentence. Every word is 'buffalo' or 'Buffalo' ‚Äî same token(s), different grammatical roles. The tokenizer can't tell them apart. Context is everything.","isOpener":false,"primary":{"tokens":[{"id":35361,"text":"Buff","displayText":"Buff","byteLen":4},{"id":12812,"text":"alo","displayText":"alo","byteLen":3},{"id":82455,"text":" buffalo","displayText":" buffalo","byteLen":8},{"id":32489,"text":" Buffalo","displayText":" Buffalo","byteLen":8},{"id":82455,"text":" buffalo","displayText":" buffalo","byteLen":8},{"id":82455,"text":" buffalo","displayText":" buffalo","byteLen":8},{"id":82455,"text":" buffalo","displayText":" buffalo","byteLen":8},{"id":32489,"text":" Buffalo","displayText":" Buffalo","byteLen":8},{"id":82455,"text":" buffalo","displayText":" buffalo","byteLen":8}],"tokenCount":9},"comparison":{"gpt2":{"tokens":[{"id":36474,"text":"Buff"},{"id":7335,"text":"alo"},{"id":45019,"text":" buffalo"},{"id":14905,"text":" Buffalo"},{"id":45019,"text":" buffalo"},{"id":45019,"text":" buffalo"},{"id":45019,"text":" buffalo"},{"id":14905,"text":" Buffalo"},{"id":45019,"text":" buffalo"}],"tokenCount":9},"bert":{"tokens":[{"id":6901,"text":"buffalo"},{"id":6901,"text":"buffalo"},{"id":6901,"text":"buffalo"},{"id":6901,"text":"buffalo"},{"id":6901,"text":"buffalo"},{"id":6901,"text":"buffalo"},{"id":6901,"text":"buffalo"},{"id":6901,"text":"buffalo"}],"tokenCount":8}},"charCount":63,"compressionRatio":7.0},{"text":"James while John had had had had had had had had had had had a better effect on the teacher","category":"edge_cases","annotation":"Another grammatically valid sentence. The word 'had' appears 11 times with the same token ID each time. Meaning depends entirely on context, not the token itself.","isOpener":false,"primary":{"tokens":[{"id":29184,"text":"James","displayText":"James","byteLen":5},{"id":1418,"text":" while","displayText":" while","byteLen":6},{"id":3842,"text":" John","displayText":" John","byteLen":5},{"id":1047,"text":" had","displayText":" had","byteLen":4},{"id":1047,"text":" had","displayText":" had","byteLen":4},{"id":1047,"text":" had","displayText":" had","byteLen":4},{"id":1047,"text":" had","displayText":" had","byteLen":4},{"id":1047,"text":" had","displayText":" had","byteLen":4},{"id":1047,"text":" had","displayText":" had","byteLen":4},{"id":1047,"text":" had","displayText":" had","byteLen":4},{"id":1047,"text":" had","displayText":" had","byteLen":4},{"id":1047,"text":" had","displayText":" had","byteLen":4},{"id":1047,"text":" had","displayText":" had","byteLen":4},{"id":1047,"text":" had","displayText":" had","byteLen":4},{"id":264,"text":" a","displayText":" a","byteLen":2},{"id":2731,"text":" better","displayText":" better","byteLen":7},{"id":2515,"text":" effect","displayText":" effect","byteLen":7},{"id":389,"text":" on","displayText":" on","byteLen":3},{"id":279,"text":" the","displayText":" the","byteLen":4},{"id":11326,"text":" teacher","displayText":" teacher","byteLen":8}],"tokenCount":20},"comparison":{"gpt2":{"tokens":[{"id":14731,"text":"James"},{"id":981,"text":" while"},{"id":1757,"text":" John"},{"id":550,"text":" had"},{"id":550,"text":" had"},{"id":550,"text":" had"},{"id":550,"text":" had"},{"id":550,"text":" had"},{"id":550,"text":" had"},{"id":550,"text":" had"},{"id":550,"text":" had"},{"id":550,"text":" had"},{"id":550,"text":" had"},{"id":550,"text":" had"},{"id":257,"text":" a"},{"id":1365,"text":" better"},{"id":1245,"text":" effect"},{"id":319,"text":" on"},{"id":262,"text":" the"},{"id":4701,"text":" teacher"}],"tokenCount":20},"bert":{"tokens":[{"id":2508,"text":"james"},{"id":2096,"text":"while"},{"id":2198,"text":"john"},{"id":2018,"text":"had"},{"id":2018,"text":"had"},{"id":2018,"text":"had"},{"id":2018,"text":"had"},{"id":2018,"text":"had"},{"id":2018,"text":"had"},{"id":2018,"text":"had"},{"id":2018,"text":"had"},{"id":2018,"text":"had"},{"id":2018,"text":"had"},{"id":2018,"text":"had"},{"id":1037,"text":"a"},{"id":2488,"text":"better"},{"id":3466,"text":"effect"},{"id":2006,"text":"on"},{"id":1996,"text":"the"},{"id":3836,"text":"teacher"}],"tokenCount":20}},"charCount":91,"compressionRatio":4.55}],"oneHotDemo":{"vocabSize":100277,"words":{"king":{"tokenIds":[10789],"isSingleToken":true,"primaryTokenId":10789,"tokenTexts":["king"]},"queen":{"tokenIds":[94214],"isSingleToken":true,"primaryTokenId":94214,"tokenTexts":["queen"]},"refrigerator":{"tokenIds":[1116,65044,859],"isSingleToken":false,"primaryTokenId":1116,"tokenTexts":["ref","riger","ator"]},"man":{"tokenIds":[1543],"isSingleToken":true,"primaryTokenId":1543,"tokenTexts":["man"]},"woman":{"tokenIds":[22803],"isSingleToken":true,"primaryTokenId":22803,"tokenTexts":["woman"]}},"dotProducts":[{"wordA":"king","wordB":"queen","tokenIdA":10789,"tokenIdB":94214,"dotProduct":0,"explanation":"Semantically similar ‚Äî but dot product is 0!"},{"wordA":"king","wordB":"refrigerator","tokenIdA":10789,"tokenIdB":1116,"dotProduct":0,"explanation":"Semantically unrelated ‚Äî also 0. Identical to king¬∑queen."},{"wordA":"king","wordB":"king","tokenIdA":10789,"tokenIdB":10789,"dotProduct":1,"explanation":"Same word ‚Äî dot product is 1. The only non-zero case."},{"wordA":"man","wordB":"woman","tokenIdA":1543,"tokenIdB":22803,"dotProduct":0,"explanation":"Another related pair ‚Äî still 0."}],"animationSequence":[{"step":1,"action":"show_vector","word":"king","narration":"Here's the one-hot vector for 'king'. It has a 1 at one position and 0s everywhere else."},{"step":2,"action":"show_vector","word":"queen","narration":"Now 'queen'. Its 1 is at a different position."},{"step":3,"action":"compute_dot_product","wordA":"king","wordB":"queen","narration":"Dot product: multiply element-by-element, then sum. The 1s are in different positions, so every product is 0. Total: 0. To this representation, 'king' and 'queen' are completely unrelated."},{"step":4,"action":"show_vector","word":"refrigerator","narration":"Now 'refrigerator' ‚Äî obviously unrelated to 'king.'"},{"step":5,"action":"compute_dot_product","wordA":"king","wordB":"refrigerator","narration":"Dot product with 'king': also 0. Exactly the same as king¬∑queen. This representation has no concept of meaning. That's the problem."},{"step":6,"action":"reveal_problem","narration":"Every pair of different words has similarity 0. There's no structure, no notion of closeness. We need something better: embeddings."}]},"vocabStats":{"tokenizers":{"GPT-4":{"encoding":"cl100k_base","vocabSize":100277,"type":"BPE (byte-level)"},"GPT-2":{"encoding":"r50k_base","vocabSize":50257,"type":"BPE (byte-level)"},"BERT (WordPiece)":{"model":"bert-base-uncased","vocabSize":30522,"type":"WordPiece"}},"exampleStats":{"totalExamples":34,"avgTokensPrimary":10.6,"avgTokensGpt2":11.7,"avgTokensBert":10.7,"maxTokensPrimary":28,"minTokensPrimary":1}}}